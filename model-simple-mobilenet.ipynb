{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Project\n",
    "---\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "Here are links to the labeled data for [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) examples to train your classifier.  These example images come from a combination of the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself.   You are welcome and encouraged to take advantage of the recently released [Udacity labeled dataset](https://github.com/udacity/self-driving-car/tree/master/annotations) to augment your training data.  \n",
    "\n",
    "Some example images for testing your pipeline on single frames are located in the `test_images` folder.  To help the reviewer examine your work, please save examples of the output from each stage of your pipeline in the folder called `ouput_images`, and include them in your writeup for the project by describing what each image shows.    The video called `project_video.mp4` is the video your pipeline should work well on.  \n",
    "\n",
    "**As an optional challenge** Once you have a working pipeline for vehicle detection, add in your lane-finding algorithm from the last project to do simultaneous lane-finding and vehicle detection!\n",
    "\n",
    "**If you're feeling ambitious** (also totally optional though), don't stop there!  We encourage you to go out and take video of your own, and show us how you would implement this project on a new video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "from skimage.feature import hog\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler=None, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        #test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also from lesson materials\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap# Iterate through list of bboxes\n",
    "    \n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to write my own code!\n",
    "\n",
    "This part is divided into:\n",
    "\n",
    "1.  Writing our own classifier\n",
    "2.  Find boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in cars and notcars\n",
    "\"\"\"\n",
    "images = glob.glob('*.jpeg')\n",
    "cars = []\n",
    "notcars = []\n",
    "for image in images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "\"\"\"\n",
    "\n",
    "#cars = glob.glob('smallset/vehicles_smallset/*.jpeg')\n",
    "#notcars = glob.glob('./smallset/non-vehicles_smallset/*.jpeg')\n",
    "\n",
    "cars = glob.glob('largeset/vehicles/*/*.png')+glob.glob('smallset/vehicles_smallset/*.jpeg')\n",
    "notcars = glob.glob('largeset/non-vehicles/*/*.png')+glob.glob('./smallset/non-vehicles_smallset/*.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_imgs = [mpimg.imread(x) for x in cars]\n",
    "notcar_imgs = [mpimg.imread(x) for x in notcars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack((car_imgs, notcar_imgs)).astype(np.float64)\n",
    "y = np.zeros(len(car_imgs)+len(notcar_imgs))\n",
    "y[:len(car_imgs)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19076, 64, 64, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import *\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobilenet_model = Model(input=base_model.input, output=base_model.get_layer('global_average_pooling2d_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_img_features(single_image):\n",
    "    \"\"\"\n",
    "    Convert to image features using mobilenet\n",
    "    \"\"\"\n",
    "    single_image = cv2.resize(single_image, (224, 224)) \n",
    "    x = keras.preprocessing.image.img_to_array(single_image)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"test_images/test6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the extract_features to a scikit learn compatible pipeline\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        all_images = np.vstack([single_img_features(single_x)\n",
    "                for single_x in X])\n",
    "        return mobilenet_model.predict(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a linear SVC \n",
    "model = Pipeline([\n",
    "    ('feats', FeatureUnion([('create_feats', FeatureCreator())])), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear svm', LinearSVC())])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_train = model.predict(X_train)\n",
    "metric = accuracy_score(y_train, yhat_train)\n",
    "print(\"Accuracy Train Rate: {}\".format(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_test = model.predict(X_test)\n",
    "metric = accuracy_score(y_test, yhat_test)\n",
    "print(\"Accuracy Test Rate: {}\".format(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find cars in a box...\n",
    "\n",
    "Goal: make a bunch of rectangles and return all rectangles which look like a car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_windows_pipeline(img, windows, model):\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        # feed into pipeline\n",
    "        prediction = model.predict([test_img])\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"test_images/test6.jpg\")\n",
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[350, 500], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.0, 0.0))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[350, 500], \n",
    "                    xy_window=(96, 96), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 550], \n",
    "                    xy_window=(128, 128), xy_overlap=(0.6, 0.6))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 720], \n",
    "                    xy_window=(192, 192), xy_overlap=(0.7, 0.7))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this one suggests we need to have some limits on x start stop!\n",
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 720], \n",
    "                    xy_window=(256, 256), xy_overlap=(0.8, 0.8))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding limits here appears to solve it - the idea is that a car that is close to you shouldn't be in the same \"lane\" \n",
    "# otherwise it might mean you're about to crash! - also it is clear that it is \n",
    "# picking up the very bottom of the black car - which might trigger a higher incidence\n",
    "# of false positives\n",
    "windows = (slide_window(image, x_start_stop=[1000, None], y_start_stop=[400, 720], \n",
    "                        xy_window=(256, 256), xy_overlap=(0.8, 0.8)) +\n",
    "              slide_window(image, x_start_stop=[0, 280], y_start_stop=[400, 720], \n",
    "                        xy_window=(256, 256), xy_overlap=(0.8, 0.8)))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 550], \n",
    "                    xy_window=(96, 96), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "# lets draw head boxes...\n",
    "heat = np.zeros_like(image[:,:,0])\n",
    "heat = add_heat(heat, hot_windows)    \n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat,1)\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "_, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax1.imshow(window_img)\n",
    "ax2.imshow(heatmap, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine all windows and draw the head map\n",
    "windows = (\n",
    "#slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 550], \n",
    "#    xy_window=(64, 64), xy_overlap=(0.0, 0.0)) +\n",
    "#slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 550], \n",
    "#    xy_window=(96, 96), xy_overlap=(0.5, 0.5)) +\n",
    "slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 550], \n",
    "    xy_window=(128, 128), xy_overlap=(0.3, 0.3)) +\n",
    "slide_window(image, x_start_stop=[None, None], y_start_stop=[400, 720], \n",
    "    xy_window=(192, 192), xy_overlap=(0.5, 0.5)) +\n",
    "slide_window(image, x_start_stop=[1000, None], y_start_stop=[400, 720], \n",
    "    xy_window=(256, 256), xy_overlap=(0.5, 0.5)) +\n",
    "slide_window(image, x_start_stop=[0, 280], y_start_stop=[400, 720], \n",
    "    xy_window=(256, 256), xy_overlap=(0.5, 0.5))\n",
    ")\n",
    "windows = (slide_window(image, x_start_stop=[730, 1280], y_start_stop=[380, 550], \n",
    "                       xy_window=(96, 96), xy_overlap=(0.75, 0.75)))\n",
    "\n",
    "hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "# lets draw head boxes...\n",
    "heat = np.zeros_like(image[:,:,0])\n",
    "heat = add_heat(heat, hot_windows)    \n",
    "# Apply threshold to help remove false positives\n",
    "heat = apply_threshold(heat,1)\n",
    "# Visualize the heatmap when displaying    \n",
    "heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "_, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax1.imshow(window_img)\n",
    "ax2.imshow(heatmap, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the label as suggested...\n",
    "labels = label(heatmap)\n",
    "draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "print(\"{} cars found\".format(len(labels)))\n",
    "plt.imshow(draw_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a full pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image, model=model):\n",
    "    # process everything based on the model pipeline used above\n",
    "    windows = (slide_window(image, x_start_stop=[730, 1280], y_start_stop=[380, 550], \n",
    "                           xy_window=(96, 96), xy_overlap=(0.75, 0.75)))\n",
    "    \n",
    "    xss = [730, 1280]\n",
    "    yss = [380, 550]\n",
    "    windows = (\n",
    "    slide_window(image, x_start_stop=xss, y_start_stop=yss, \n",
    "        xy_window=(64, 64), xy_overlap=(0.6, 0.6)) +\n",
    "    slide_window(image, x_start_stop=xss, y_start_stop=yss, \n",
    "        xy_window=(96, 96), xy_overlap=(0.75, 0.75)) \n",
    "    #slide_window(image, x_start_stop=xss, y_start_stop=yss, \n",
    "    #    xy_window=(128, 128), xy_overlap=(0.6, 0.6)) +\n",
    "    #slide_window(image, x_start_stop=xss, y_start_stop=yss, \n",
    "    #    xy_window=(192, 192), xy_overlap=(0.7, 0.7)) +\n",
    "    #slide_window(image, x_start_stop=[1000, None], y_start_stop=yss, \n",
    "    #    xy_window=(256, 256), xy_overlap=(0.0, 0.0)) +\n",
    "    #slide_window(image, x_start_stop=[0, 280], y_start_stop=yss, \n",
    "    #    xy_window=(256, 256), xy_overlap=(0.0, 0.0))\n",
    "    )\n",
    "\n",
    "\n",
    "    hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "    window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "    # lets draw head boxes...\n",
    "    heat = np.zeros_like(image[:,:,0])\n",
    "    heat = add_heat(heat, hot_windows)    \n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat,1)\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)    \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(process_image(image, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run for all images in test\n",
    "test_images = glob.glob('./test_images/test*.jpg')\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16,14))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, im in enumerate(test_images):\n",
    "    axs[i].imshow(process_image(mpimg.imread(im)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_video_20170813.mp4\n",
      "[MoviePy] Writing video test_video_20170813.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████▉  | 38/39 [00:18<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_video_20170813.mp4 \n",
      "\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "#test_out_file = 'test_video_20170813.mp4'\n",
    "#clip_test = VideoFileClip('test_video.mp4')\n",
    "#clip_test_out = clip_test.fl_image(process_image)\n",
    "#%time clip_test_out.write_videofile(test_out_file, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video project_video_20170813.mp4\n",
      "[MoviePy] Writing video project_video_20170813.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1260/1261 [11:59<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project_video_20170813.mp4 \n",
      "\n",
      "Wall time: 12min 2s\n"
     ]
    }
   ],
   "source": [
    "#test_out_file = 'project_video_20170813.mp4'\n",
    "#clip_test = VideoFileClip('project_video.mp4')\n",
    "#clip_test_out = clip_test.fl_image(process_image)\n",
    "#%time clip_test_out.write_videofile(test_out_file, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
