{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Project\n",
    "---\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "Here are links to the labeled data for [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) examples to train your classifier.  These example images come from a combination of the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself.   You are welcome and encouraged to take advantage of the recently released [Udacity labeled dataset](https://github.com/udacity/self-driving-car/tree/master/annotations) to augment your training data.  \n",
    "\n",
    "Some example images for testing your pipeline on single frames are located in the `test_images` folder.  To help the reviewer examine your work, please save examples of the output from each stage of your pipeline in the folder called `ouput_images`, and include them in your writeup for the project by describing what each image shows.    The video called `project_video.mp4` is the video your pipeline should work well on.  \n",
    "\n",
    "**As an optional challenge** Once you have a working pipeline for vehicle detection, add in your lane-finding algorithm from the last project to do simultaneous lane-finding and vehicle detection!\n",
    "\n",
    "**If you're feeling ambitious** (also totally optional though), don't stop there!  We encourage you to go out and take video of your own, and show us how you would implement this project on a new video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "from skimage.feature import hog\n",
    "import glob\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "cars = glob.glob('largeset/vehicles/*/*.png')+glob.glob('smallset/vehicles_smallset/*.jpeg')\n",
    "notcars = glob.glob('largeset/non-vehicles/*/*.png')+glob.glob('./smallset/non-vehicles_smallset/*.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the extract_features to a scikit learn compatible pipeline\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=11, \n",
    "                        pix_per_cell=16, cell_per_block=2, hog_channel='ALL',\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "        self.color_space  = color_space\n",
    "        self.spatial_size = spatial_size\n",
    "        self.hist_bins = hist_bins\n",
    "        self.orient = orient\n",
    "        self.pix_per_cell = pix_per_cell\n",
    "        self.cell_per_block = cell_per_block\n",
    "        self.hog_channel = hog_channel\n",
    "        self.spatial_feat = spatial_feat\n",
    "        self.hist_feat = hist_feat\n",
    "        self.hog_feat = hog_feat\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [single_img_features(single_x, self.color_space, self.spatial_size, self.hist_bins, \n",
    "                                   self.orient, self.pix_per_cell, \n",
    "                                   self.cell_per_block, self.hog_channel, self.spatial_feat, \n",
    "                                   self.hist_feat, self.hog_feat)\n",
    "                for single_x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_imgs = [mpimg.imread(x) for x in cars]\n",
    "notcar_imgs = [mpimg.imread(x) for x in notcars]\n",
    "\n",
    "X = np.vstack((car_imgs, notcar_imgs)).astype(np.float64)\n",
    "y = np.zeros(len(car_imgs)+len(notcar_imgs))\n",
    "y[:len(car_imgs)] = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('create_feats', FeatureCreator(cell_per_block=2, color_space='RGB', hist_bins=32,\n",
       "        hist_feat=True, hog_channel='ALL', hog_feat=True, orient=11,\n",
       "        pix_per_cell=16, spatial_feat=True, spatial_size=(32, 32)))],\n",
       "       transf...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a linear SVC \n",
    "model = Pipeline([\n",
    "    ('feats', FeatureUnion([('create_feats', FeatureCreator())])), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear svm', LinearSVC())])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model.predict(X_train)\n",
    "metric = accuracy_score(y_train, yhat_train)\n",
    "print(\"Accuracy Train Rate: {}\".format(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_test = model.predict(X_test)\n",
    "metric = accuracy_score(y_test, yhat_test)\n",
    "print(\"Accuracy Test Rate: {}\".format(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find cars in a box...\n",
    "\n",
    "Goal: make a bunch of rectangles and return all rectangles which look like a car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_windows_pipeline(img, windows, model):\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        # feed into pipeline\n",
    "        prediction = model.predict([test_img])\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a full pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image, model=model):\n",
    "    # process everything based on the model pipeline used above\n",
    "    windows = (slide_window(image, x_start_stop=[730, 1280], y_start_stop=[380, 550], \n",
    "                           xy_window=(96, 96), xy_overlap=(0.75, 0.75)))\n",
    "    \n",
    "    hot_windows = search_windows_pipeline(image, windows, model)  \n",
    "    window_img = draw_boxes(image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "    \n",
    "    # lets draw head boxes...\n",
    "    heat = np.zeros_like(image[:,:,0])\n",
    "    heat = add_heat(heat, hot_windows)    \n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heat,1)\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(image), labels)    \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(process_image(image, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for all images in test\n",
    "test_images = glob.glob('./test_images/test*.jpg')\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16,14))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, im in enumerate(test_images):\n",
    "    axs[i].imshow(process_image(mpimg.imread(im)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_out_file = 'test_video_20170813.mp4'\n",
    "#clip_test = VideoFileClip('test_video.mp4')\n",
    "#clip_test_out = clip_test.fl_image(process_image)\n",
    "#%time clip_test_out.write_videofile(test_out_file, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_out_file = 'project_video_20170813.mp4'\n",
    "#clip_test = VideoFileClip('project_video.mp4')\n",
    "#clip_test_out = clip_test.fl_image(process_image)\n",
    "#%time clip_test_out.write_videofile(test_out_file, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
